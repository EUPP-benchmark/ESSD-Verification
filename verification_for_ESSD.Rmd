---
title: "Benchmark for ESSD"
author: "EUMETNET verification team"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
---

# Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 8, fig.height = 4)
```
This is an attempt at automatically installing dependencies.
A better integrated way (e.g. `ESSD-verification` as a package) may be implemented at a later stage.

```{r libraries}
dependencies <- c('dplyr', 'ncdf4', 'ncdf4.helpers', 'tibble', 'tidync', 'ggplot2', 'tidyr', 'SpecsVerification', 'verification', 'zoo', 'rmarkdown', 'knitr')
for (pkg in dependencies) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
}

source("read_data.R")
source("verif_functions.R")

library(ggplot2)
```


# Data

Before you load the data, please make sure that your path points to the directory downloaded from [https://cloud.meteo.be/s/BzYNiaEYoWX8gnA](`cloud.meteo.be`).

## Verifying observations and auxiliary data

```{r data}
## set the path to your data directory
path <- "data"

obsfile <- list.files(path, "observations.nc", full.names = TRUE)
# exclude old versions from evaluation that are have been superseeded
fcfiles <- list.files(path, ".nc$", full.names = TRUE) %>%
  grep(obsfile, ., invert = TRUE, value = TRUE) %>%
  grep("ZAMG_EMOS-v1.0.nc", ., invert = TRUE, value = TRUE) %>%
  grep("KIT_simple-NN_v1.0.nc", ., invert = TRUE, value = TRUE) %>%
  grep("ECMWF.*v1.0.nc", ., invert = TRUE, value = TRUE) %>%
  grep("Bielefeld.*1.0", ., invert = TRUE, value = TRUE) %>%
  grep("MetOffice.*1.0", ., invert = TRUE, value = TRUE)

## read observations
obs <- obsfile %>% 
  tidync::tidync("t2m") %>%
  tidync::hyper_tibble(na.rm = FALSE) %>%
  dplyr::mutate(
    time = as.POSIXct("1970-01-01", tz = 'UTC') + 
      as.difftime(time, units = "secs")
  ) %>%
  dplyr::rename(obs = t2m)

## read station metadata and join in 
## the model orography and percentage missing
stations <- obsfile %>%
  tidync::tidync("D0") %>% 
  tidync::hyper_tibble(na.rm = FALSE) %>%
  dplyr::left_join(
    read.csv(paste0(path, "/model_orography_on_stations.csv")), 
    by = "station_id"
  ) %>%
  dplyr::left_join(
    read.csv(paste0(path, "/percentage_of_nan_in_t2m.csv")) %>%
      dplyr::rename(percentage_missing = X0), 
    by = "station_id"
  ) %>% 
  dplyr::left_join(
    read.csv(paste0(path, "/percentage_of_nan_in_t2m_bvs.csv")) %>%
      dplyr::rename(fraction_available = fraction) %>%
      dplyr::select(-X), 
    by = c("station_id", "station_name")
  )

```

The observations are read into a data frame with four columns. 
The observed values are stored in the column named `obs`.

```{r obs}
obs
```


Additional station metadata is extracted from the `*observations.nc` file and the `*.csv` files.
```{r stations}
stations
```

## Forecasts

The forecasts follow the same format, with the exception that the ensemble or quantile forecast is stored as an matrix column. 
This matrix column contains the 51 realizations (quantiles). 

```{r fcst}
read_file(fcfiles[1])
```
Some of the mis-specified files may not be sorted as expected, as shown by the correlation of ensemble mean and observed mean at stations.
Datasets with issues are excluded from further analysis, this currently concerns the `1_ESSD-benchmark_Bielefeld-University_AR-EMOS_v1.0` and `1_ESSD-benchmark_MetOffice_IMPROVER-reliabilitycalibration-v1.3.1_v1.0` datasets.


## Scores

Scores are computed by joining the forecast and observation data frames on the common dimensions.
This ensures that the ordering is correct (if correctly specified).
Simple file-based caching is now done in `compute_scores`. 
If you want to recompute the scores, you have to delete all `*.rds` files in your data directory.
```{r scores}
scores <- lapply(fcfiles, compute_scores, obs = obs) %>%
  dplyr::bind_rows()
```


## Common subset for analysis
First, we need to identify the common subset across all datasets in the analysis.
To do this we can count the number of `source`s per `station_id`, `time`, and `step`.
The variability with time is due to the variability in verifying observations, rather than postprocessed forecasts.

```{r common}
common <- scores %>%
  tidyr::drop_na() %>% 
  dplyr::count(station_id, time, step) %>% # count how many sources there are
  dplyr::ungroup() %>% 
  dplyr::filter(n == max(n)) %>%
  dplyr::select(-n)

## subset scores
scores <- common %>%
  dplyr::left_join(scores, by = c("station_id", "time", "step")) 

common %>% 
  dplyr::count(time, step) %>%
  ggplot(aes(x = step, y = time, fill = n)) + 
  geom_tile() + 
  theme_light() + 
  scale_fill_viridis_c()
```
## Overall scores and uncertainty

```{r uncertainty, fig.height = 5}
scores_array <- scores %>% 
  dplyr::select(source, station_id, time, step, crps) %>%
  tidyr::pivot_wider(names_from = "source", values_from = "crps") %>%
  dplyr::select(-station_id, -time, -step)

scores_diff <- sapply(
  scores_array, 
  function(x) {
    sapply(scores_array, function(y) SpecsVerification::ScoreDiff(x, y), simplify = 'array')
  }, 
  simplify = 'array'
) 

scores_txt <- scores_diff %>%
  round(3) %>% 
  apply(
    2:3, 
    function(x) {
      paste0(x["score.diff"], " (", x["L"], " to ", x["U"], ")")
    }) %>%
  as.vector()

source_tags <- tibble::tibble(
  source = colnames(scores_diff) 
) %>%
  dplyr::mutate(source = gsub("EMOS-v1.1", "EMOS_v1.1", source)) %>%
  tidyr::separate(source, into = c("source", "model", "version"), sep = "_") %>%
  dplyr::mutate(source_tag = paste(source, version)) %>%
  dplyr::pull(source_tag)

tidyr::expand_grid(
  source1 = source_tags, 
  source2 = source_tags
) %>%
  dplyr::mutate(
    p_value = as.vector(scores_diff["p.value",,]), 
    diff = as.vector(scores_diff["score.diff",,]),
    text = scores_txt, 
    text = gsub("\\(", "\n(", text)
  ) %>%
  ggplot(aes(x = source1, y = source2, fill = diff)) + 
  geom_tile() + 
  geom_text(aes(label = text), size = 1.1) + 
  scale_fill_gradient2(low = "darkgreen", high = "purple", na.value = NA) + 
  theme_light() + 
  theme(
    legend.position = "none", 
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)
  ) + 
  labs(x = "", y = "") + 
  ggtitle(
    "Difference in mean CRPS",
    subtitle = "All differences are significant at 95% level according to `SpecsVerification::ScoreDiff`"
  )
```

## Score aggregates
Aggregates can be computed by using `group_by` and `summarize`.

```{r score_mn}
scores_mn <- scores %>%
  dplyr::group_by(source, step) %>%
  dplyr::summarize(
    bias = mean(mn - obs),
    mae = mean(abs(mn - obs)), 
    rmse = sqrt(mean((mn - obs)**2)), 
    sd = mean(sd), 
    crps = mean(crps), 
    .groups = "drop"
  ) %>% 
  dplyr::mutate(
    s2e = sd / rmse
  ) %>%
  dplyr::mutate(source = gsub("EMOS-v1.1", "EMOS_v1.1", source)) %>%
  tidyr::separate(source, into = c("source", "model", "version"), sep = "_") %>%
  dplyr::mutate(source_tag = paste(source, version)) 

scores_map <-  scores %>%
  dplyr::group_by(source, station_id) %>%
  dplyr::summarize(
    bias = mean(mn - obs),
    mae = mean(abs(mn - obs)), 
    rmse = sqrt(mean((mn - obs)**2)), 
    sd = mean(sd), 
    crps = mean(crps), 
    mn = mean(mn), 
    obs = mean(obs),
    .groups = "drop"
  ) %>%
  dplyr::mutate(
    s2e = sd / rmse
  ) %>%
  left_join(stations, by = "station_id") %>%
  dplyr::mutate(source = gsub("EMOS-v1.1", "EMOS_v1.1", source)) %>%
  tidyr::separate(source, into = c("source", "model", "version"), sep = "_") %>%
  dplyr::mutate(source_tag = paste(source, version)) 

## add in CRPSS for scores_map
scores_map <- scores_map %>% 
  dplyr::filter(source_tag == "test forecasts") %>%
  dplyr::select(station_id, crps) %>%
  dplyr::rename(benchmark = crps) %>%
  dplyr::full_join(scores_map, by = 'station_id') %>%
  dplyr::mutate(crpss = 1 - crps / benchmark)

ranks <-  scores %>%
  group_by(source, rank) %>%
  count() %>%
  dplyr::mutate(source = gsub("EMOS-v1.1", "EMOS_v1.1", source)) %>%
  tidyr::separate(source, into = c("source", "model", "version"), sep = "_") %>%
  dplyr::mutate(source_tag = paste(source, version)) 

colours <- scores_mn %>%
  dplyr::select(source, version, source_tag) %>%
  dplyr::distinct() %>%
  dplyr::filter(source_tag != "test forecasts") %>%
  dplyr::mutate(
    source = as.numeric(factor(source))
  ) %>%
  dplyr::group_by(source) %>%
  dplyr::mutate(
    version = as.numeric(factor(version)),
    version = version / max(version)
  ) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(
    colour = hcl(source / max(source) * 360, l = 110 - 60 * version, c = 70)
  )

source_colours <- setNames(
  c("#000000", colours$colour), 
  c("test forecasts", colours$source_tag)
)

source_shapes <- setNames(
  c(21, 21 + colours$source %% 4), 
  c("test forecasts", colours$source_tag)
)


```


```{r check, eval = FALSE}
map_cor <- scores_map %>% 
  group_by(source_tag) %>%
  summarize(cor = cor(mn, obs))

okish <- map_cor %>%
  dplyr::filter(cor > 0.7, !grepl("MetOffice", source_tag)) %>%
  dplyr::pull("source_tag")

map_cor
```

# Results

## Scores by lead time
We start with a plot of scores by lead time.

```{r leadtime}
scores_mn %>%
  tidyr::pivot_longer(-c(source_tag, source, model, version, step)) %>%
  ggplot(aes(x = step, y = value, col = source_tag)) + 
  geom_line() + 
  facet_wrap(~ name, scales= "free_y", nrow = 3) + 
  theme_light() + 
  scale_color_manual(values = source_colours, name = "")
```

```{r leadtime-skill}
scores_mn %>%
  tidyr::pivot_longer(-c(source_tag, source, model, version, step)) %>%
  dplyr::select(-c(source, model, version)) %>%
  tidyr::pivot_wider(names_from = source_tag, values_from = value) %>%
  tidyr::pivot_longer(-c(step, name, `test forecasts`), names_to = "source_tag") %>%
  dplyr::mutate(skill = 1 - value / `test forecasts`) %>%
  dplyr::filter(name %in% c("crps", "mae", "rmse")) %>%
  ggplot(aes(x = step, y = skill, col = source_tag)) + 
  geom_hline(yintercept = 0, lty = 2) + 
  geom_line() + 
  facet_wrap(~ name, nrow = 3) + 
  theme_light() + 
  scale_color_manual(values = source_colours, name = "", drop = TRUE) + 
  labs(y = "Skill against altitude-corrected ECMWF DMO")
```

## Calibration
Rank histograms are produced as follows.

```{r rank-histogram}
 ranks %>%
  dplyr::filter(source_tag != "test forecasts") %>%
  ggplot(aes(x = rank, y = n, fill = source_tag)) + 
  geom_bar(stat = "identity") +  
  facet_wrap(~source_tag) + 
  theme_void() + 
  scale_fill_manual(values = source_colours) + 
  theme(legend.position = 'none', panel.border = element_rect(fill = NA)) 
```

## Score maps
A map of the bias can be produced as follows.

```{r map-bias, fig.height = 9}
scores_map %>%
  dplyr::filter(source_tag != "test forecasts") %>%
  ggplot(aes(x = longitude, y = latitude)) + 
  borders(
    xlim = range(scores_map$longitude), 
    ylim = range(scores_map$latitude)
  ) + 
  geom_point(aes(fill = bias), pch = 21, size = 2) + 
  facet_wrap(~source_tag) + 
  scale_fill_gradient2(low = "darkblue", high = "darkred") + 
  coord_fixed(
    ratio = 1/ cos(50/180*pi),
    xlim = range(scores_map$longitude), 
    ylim = range(scores_map$latitude)
  ) +
  theme_light()
```

```{r map-crpss, fig.height = 9}
scores_map %>%
  dplyr::select(station_id, longitude, latitude, source_tag, crps) %>%
  tidyr::pivot_wider(names_from = "source_tag", values_from = "crps") %>%
  tidyr::pivot_longer(
    -c(station_id, longitude, latitude, `test forecasts`), 
    names_to = "source_tag"
  ) %>%
  dplyr::mutate(skill = 1 - value / `test forecasts`) %>%
  ggplot(aes(x = longitude, y = latitude)) + 
  borders(
    xlim = range(scores_map$longitude), 
    ylim = range(scores_map$latitude)
  ) + 
  geom_point(aes(fill = skill), pch = 21, size = 2) + 
  facet_wrap(~source_tag) + 
  scale_fill_gradient2(low = "purple", high = "darkgreen") + 
  coord_fixed(
    ratio = 1/ cos(50/180*pi),
    xlim = range(scores_map$longitude), 
    ylim = range(scores_map$latitude)
  ) +
  theme_light()
```

What is the best postprocessing method per station (measured in average CRPS)?

```{r best, fig.height = 8}
dd <- scores_map %>% 
  group_by(station_id) %>%
  slice_min(crps)

dd %>%
  ggplot(aes(x = longitude, y = latitude, fill = source_tag)) + 
  borders(
    xlim = range(scores_map$longitude), 
    ylim = range(scores_map$latitude)
  ) + 
  geom_point(aes(pch = source_tag), size = 3) + 
  coord_fixed(
    ratio = 1/ cos(50/180*pi),
    xlim = range(scores_map$longitude), 
    ylim = range(scores_map$latitude)
  ) +
  theme_light() + 
  theme(panel.border = element_rect(fill = NA)) + 
  scale_fill_manual(
    name = "",
    values = source_colours
  ) +
  scale_shape_manual(
    name = "",
    values = source_shapes
  )

  
```


## Scores in dependence of auxiliary data
Alternatively, we may want to investigate bias in dependence of altitude, altitude difference to model orography, or percentage missing.

```{r scores-altitude}
scores_map %>%
  tidyr::pivot_longer(c(bias, crps, crpss, sd, s2e), names_to = "score") %>%
  ggplot(aes(x = altitude, y = value, col = source_tag)) + 
  geom_point() + 
  facet_grid(score ~ source_tag, scales = "free_y") + 
  theme_light() + 
  scale_color_manual(values = source_colours) + 
  labs(x = "Altitude of station", y = "") + 
  theme(legend.position = "none")
```
```{r scores-altitude-difference}
scores_map %>%
  tidyr::pivot_longer(c(bias, crps, crpss, sd, s2e), names_to = "score") %>%
  ggplot(aes(x = altitude - orography, y = value, col = source_tag)) + 
  geom_point() + 
  facet_grid(score ~ source_tag, scales = "free_y") + 
  theme_light() + 
  scale_color_manual(values = source_colours) + 
  labs(x = "Difference between station altitude and model orography", y = "") + 
  theme(legend.position = "none")
```

```{r scores-missing}
scores_map %>%
  tidyr::pivot_longer(c(bias, crps, crpss, sd, s2e), names_to = "score") %>%
  ggplot(aes(x = percentage_missing, y = value, col = source_tag)) + 
  geom_point() + 
  facet_grid(score ~ source_tag, scales = "free_y") + 
  theme_light() + 
  labs(x = "Percentage of missing values", y = "") + 
  scale_color_manual(values = source_colours) + 
  theme(legend.position = "none")
```

```{r scores-training}
scores_map %>%
  tidyr::pivot_longer(c(bias, crps, crpss, sd, s2e), names_to = "score") %>%
  ggplot(aes(x = fraction_available, y = value, col = source_tag)) + 
  geom_point() + 
  facet_grid(score ~ source_tag, scales = "free_y") + 
  theme_light() + 
  labs(x = "Fraction of observations in training set", y = "") + 
  scale_color_manual(values = source_colours) + 
  theme(legend.position = "none")
```
```{r time}
scores_ty <- scores %>%
  dplyr::group_by(source, time) %>%
  dplyr::summarize(
    bias = mean(mn - obs),
    mae = mean(abs(mn - obs)), 
    rmse = sqrt(mean((mn - obs)**2)), 
    sd = mean(sd), 
    crps = mean(crps), 
    .groups = "drop"
  ) %>% 
  dplyr::mutate(
    s2e = sd / rmse
  ) %>%
  dplyr::mutate(source = gsub("EMOS-v1.1", "EMOS_v1.1", source)) %>%
  tidyr::separate(source, into = c("source", "model", "version"), sep = "_") %>%
  dplyr::mutate(source_tag = paste(source, version)) 


scores_ty %>%
  tidyr::pivot_longer(-c(source_tag, source, model, version, time)) %>%
  ggplot(aes(x = time, y = value, col = source_tag)) + 
  geom_line() + 
  facet_wrap(~ name, scales= "free_y", nrow = 3) + 
  theme_light() + 
  scale_color_manual(values = source_colours, name = "")
```

